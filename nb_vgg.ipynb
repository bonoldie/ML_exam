{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pickle\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, adjusted_rand_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Initialing compute device (use GPU if available).\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'[torch] using {device}')\n",
    "\n",
    "plt.ion()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "raw_dataset = np.load(\"dataset_food101tiny.zip\", allow_pickle=True)\n",
    "\n",
    "dataset = {\n",
    "    \"train\": {\n",
    "        \"data\": [],\n",
    "        \"names\": [],\n",
    "        \"labels\": [],\n",
    "        \"unique_labels\": [],\n",
    "    },\n",
    "    \"valid\": {\n",
    "        \"data\": [],\n",
    "        \"names\": [],\n",
    "        \"labels\": [],\n",
    "        \"unique_labels\": [],\n",
    "    },\n",
    "}\n",
    "\n",
    "images_shape = (224, 224)\n",
    "\n",
    "# For each image we have the path from which we extract the name and the label of the image\n",
    "for dsKey in raw_dataset.keys():\n",
    "    splittedKey = dsKey.split(\"/\")\n",
    "\n",
    "    img_type = splittedKey[2]\n",
    "    img_label = splittedKey[3]\n",
    "    img_name = splittedKey[4]\n",
    "\n",
    "    img = Image.open(io.BytesIO(raw_dataset[dsKey]))\n",
    "    img = ImageOps.fit(img, images_shape, Image.Resampling.LANCZOS).convert(\"RGB\")\n",
    "\n",
    "    img_array = np.asarray(img)\n",
    "\n",
    "    dataset[img_type][\"data\"].append(img_array)\n",
    "    dataset[img_type][\"names\"].append(img_name)\n",
    "    dataset[img_type][\"labels\"].append(img_label)\n",
    "\n",
    "for img_type in dataset.keys():\n",
    "    dataset[img_type][\"data\"] = np.asarray(dataset[img_type][\"data\"])\n",
    "    dataset[img_type][\"names\"] = np.asarray(dataset[img_type][\"names\"])\n",
    "\n",
    "    dataset[img_type][\"unique_labels\"], dataset[img_type][\"labels\"] = np.unique(\n",
    "        np.asarray(dataset[img_type][\"labels\"]), return_inverse=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction using VGG\n",
    "\n",
    "Normalization mean and standard deviation are [here](https://pytorch.org/hub/pytorch_vision_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_std = [0.229, 0.224, 0.225]\n",
    "normalization_mean = [0.485, 0.456, 0.406]\n",
    "\n",
    "loader = transforms.Compose(\n",
    "    [\n",
    "        # transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=normalization_mean, std=normalization_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vgg_out = {\"train\": [], \"valid\": []}\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.vgg16(weights=models.VGG16_Weights.DEFAULT).features.to(device)\n",
    "\n",
    "for img_type in dataset.keys():\n",
    "    vgg_out[img_type] = []\n",
    "\n",
    "    for image_idx in range(dataset[img_type][\"data\"].shape[0]):\n",
    "        loaded_image = (\n",
    "            loader(dataset[img_type][\"data\"][image_idx, :]).unsqueeze(0).to(device)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            res = model(loaded_image)\n",
    "        features = res.data.detach().cpu().numpy().flatten()\n",
    "        print(f\"Extracting feature: {image_idx}/{dataset[img_type]['data'].shape[0]}\")\n",
    "\n",
    "        vgg_out[img_type].append(features)\n",
    "\n",
    "    vgg_out[img_type] = np.asarray(vgg_out[img_type])\n",
    "    print(vgg_out[img_type].shape)\n",
    "\n",
    "pickle.dump(vgg_out, open(\".pkl/vgg_out.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction using PCA, LDA and t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each method is tested with a set of numer of components\n",
    "n_components_to_test = {\n",
    "    \"PCA\": [3, 10, 50, 100, 200, 500, 1200],\n",
    "    \"LDA\": [3, 5, 7, 9],\n",
    "    \"TSNE\": [2, 3],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preload env\n",
    "vgg_out = pickle.load(open(\".pkl/vgg_out.pkl\", \"rb\"))\n",
    "\n",
    "# Results to compare the methods the number of component changes\n",
    "results_PCA = []\n",
    "results_LDA = []\n",
    "\n",
    "PCAs_instances = {}\n",
    "LDAs_instances = {}\n",
    "TSNEs_instances = {}\n",
    "\n",
    "PCAs_results = {\n",
    "    \"train\": {},\n",
    "    \"valid\": {},\n",
    "}\n",
    "\n",
    "LDAs_results = {\n",
    "    \"train\": {},\n",
    "    \"valid\": {},\n",
    "}\n",
    "\n",
    "TSNEs_results = {\n",
    "    \"train\": {},\n",
    "    \"valid\": {},\n",
    "}\n",
    "\n",
    "for n_components in n_components_to_test[\"PCA\"]:\n",
    "    print(f'[PCA] Extracting features (# components:{n_components})')\n",
    "    \n",
    "    PCAs_instances[n_components] = []\n",
    "\n",
    "    PCAs_results[\"train\"][n_components] = []\n",
    "    PCAs_results[\"valid\"][n_components] = []\n",
    "\n",
    "    PCA_instance = PCA(n_components=n_components)\n",
    "\n",
    "    PCA_instance.fit(vgg_out[\"train\"])\n",
    "\n",
    "    PCAs_results[\"train\"][n_components] = PCA_instance.transform(vgg_out[\"train\"])\n",
    "    PCAs_results[\"valid\"][n_components] = PCA_instance.transform(vgg_out[\"valid\"])\n",
    "\n",
    "    PCAs_instances[n_components] = PCA_instance\n",
    "    \n",
    "    results_PCA.append(\n",
    "        {\n",
    "            \"METHOD\": \"PCA\",\n",
    "            \"# Components\": n_components,\n",
    "            \"CHANNEL\": \"RGB\",\n",
    "            \"Explained Variance Ratio\": np.sum(\n",
    "                PCA_instance.explained_variance_ratio_, axis=0\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "for n_components in n_components_to_test[\"LDA\"]:\n",
    "    print(f'[LDA] Extracting features (# components:{n_components})')\n",
    "    \n",
    "    LDAs_instances[n_components] = []\n",
    "\n",
    "    LDAs_results[\"train\"][n_components] = []\n",
    "    LDAs_results[\"valid\"][n_components] = []\n",
    "\n",
    "    LDA_instance = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "\n",
    "    LDA_instance.fit(vgg_out[\"train\"], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "    LDAs_results[\"train\"][n_components] = LDA_instance.transform(vgg_out[\"train\"])\n",
    "    LDAs_results[\"valid\"][n_components] = LDA_instance.transform(vgg_out[\"valid\"])\n",
    "\n",
    "    LDAs_instances[n_components] = LDA_instance\n",
    "\n",
    "    results_LDA.append(\n",
    "        {\n",
    "            \"METHOD\": \"LDA\",\n",
    "            \"# Components\": n_components,\n",
    "            \"CHANNEL\": \"RGB\",\n",
    "            \"Explained Variance Ratio\": np.sum(\n",
    "                LDA_instance.explained_variance_ratio_, axis=0\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "for n_components in n_components_to_test[\"TSNE\"]:\n",
    "    print(f'[t-SNE] Extracting features (# components:{n_components})')\n",
    "\n",
    "    TSNEs_instances[n_components] = []\n",
    "\n",
    "    TSNEs_results[\"train\"][n_components] = []\n",
    "\n",
    "    TSNE_instance_train = TSNE(n_components=n_components, verbose=1, n_iter=3000)\n",
    "    TSNE_instance_valid = TSNE(n_components=n_components, verbose=1, n_iter=3000)\n",
    "\n",
    "    TSNEs_results[\"train\"][n_components] = TSNE_instance_train.fit_transform(\n",
    "        LDAs_results[\"train\"][7]\n",
    "    )\n",
    "\n",
    "    TSNEs_results[\"valid\"][n_components] = TSNE_instance_valid.fit_transform(\n",
    "        LDAs_results[\"valid\"][7]\n",
    "    )\n",
    "\n",
    "    TSNEs_instances[n_components] = [TSNE_instance_train, TSNE_instance_valid]\n",
    "\n",
    "\n",
    "# Pandas tables\n",
    "df_results_PCA = pd.DataFrame(results_PCA)\n",
    "df_results_LDA = pd.DataFrame(results_LDA)\n",
    "\n",
    "def highlight_cells(val):\n",
    "    color = \"\"\n",
    "    if val > 0.80:\n",
    "        color = \"background-color: lightgreen; color: black; font-weight: bold\"\n",
    "    elif val < 0.80:\n",
    "        color = \"background-color: lightcoral; color: black; font-weight: bold\"\n",
    "    return color\n",
    "\n",
    "# Apply the style\n",
    "df_results_PCA_styled = (\n",
    "    df_results_PCA.style.map(highlight_cells, subset=[\"Explained Variance Ratio\"])\n",
    "    .set_caption(\"PCA Results\")\n",
    "    .set_properties(**{\"text-align\": \"center\"})\n",
    ")\n",
    "\n",
    "df_results_LDA_styled = (\n",
    "    df_results_LDA.style.map(highlight_cells, subset=[\"Explained Variance Ratio\"])\n",
    "    .set_caption(\"LDA Results\")\n",
    "    .set_properties(**{\"text-align\": \"center\"})\n",
    ")\n",
    "\n",
    "\n",
    "pickle.dump(PCAs_results, open(\".pkl/vgg_pca_out.pkl\", \"wb\"))\n",
    "pickle.dump(LDAs_results, open(\".pkl/vgg_lda_out.pkl\", \"wb\"))\n",
    "pickle.dump(TSNEs_results, open(\".pkl/vgg_tsne_out.pkl\", \"wb\"))\n",
    "\n",
    "display(df_results_PCA_styled)\n",
    "display(df_results_LDA_styled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D/3D Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_out = pickle.load(open(\".pkl/vgg_out.pkl\", \"rb\"))\n",
    " \n",
    "PCAs_results = pickle.load(open(\".pkl/vgg_pca_out.pkl\", \"rb\"))\n",
    "LDAs_results = pickle.load(open(\".pkl/vgg_lda_out.pkl\", \"rb\"))\n",
    "TSNEs_results = pickle.load(open(\".pkl/vgg_tsne_out.pkl\", \"rb\"))\n",
    "\n",
    "tSNE_fig_2D = plt.figure()\n",
    "tSNE_3D = tSNE_fig_2D.add_subplot()\n",
    "\n",
    "for i in range(len(dataset[\"train\"][\"unique_labels\"])):\n",
    "    classIdxs = dataset[\"train\"][\"labels\"] == i\n",
    "\n",
    "    tsne_features = TSNEs_results[\"train\"][2][classIdxs, :]\n",
    "\n",
    "    tSNE_3D.set_label(dataset[\"train\"][\"unique_labels\"][i])\n",
    "    tSNE_3D.scatter(\n",
    "        tsne_features[:, 0],\n",
    "        tsne_features[:, 1],\n",
    "        marker=\".\",\n",
    "        label=dataset[\"train\"][\"unique_labels\"][i],\n",
    "    )\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# 3D plot\n",
    "tSNE_fig_3D = plt.figure()\n",
    "tSNE_3D = tSNE_fig_3D.add_subplot(projection=\"3d\")\n",
    "\n",
    "for i in range(len(dataset[\"train\"][\"unique_labels\"])):\n",
    "    classIdxs = dataset[\"train\"][\"labels\"] == i\n",
    "\n",
    "    tsne_features = TSNEs_results[\"train\"][3][classIdxs, :]\n",
    "\n",
    "    tSNE_3D.scatter(\n",
    "        tsne_features[:, 0],\n",
    "        tsne_features[:, 1],\n",
    "        tsne_features[:, 2],\n",
    "        marker=\".\",\n",
    "        label=dataset[\"train\"][\"unique_labels\"][i],\n",
    "    )\n",
    "\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_out = pickle.load(open(\".pkl/vgg_out.pkl\", \"rb\"))\n",
    " \n",
    "PCAs_results = pickle.load(open(\".pkl/vgg_pca_out.pkl\", \"rb\"))\n",
    "LDAs_results = pickle.load(open(\".pkl/vgg_lda_out.pkl\", \"rb\"))\n",
    "TSNEs_results = pickle.load(open(\".pkl/vgg_tsne_out.pkl\", \"rb\"))\n",
    "\n",
    "# Number of neighbors to test\n",
    "k_to_test = {\n",
    "    \"VGG\": [3, 5, 9, 15, 21, 55, 111, 251],\n",
    "    \"PCA\": [3, 5, 9, 15, 21, 55, 111, 251],\n",
    "    \"LDA\": [3, 5, 9, 15, 21, 55, 111, 251],\n",
    "}\n",
    "\n",
    "KNN_VGG_stats = []\n",
    "KNN_PCA_stats = []\n",
    "KNN_LDA_stats = []\n",
    "\n",
    "for k_idx, k in enumerate(k_to_test[\"VGG\"]):\n",
    "\n",
    "    KNN_VGG_stats.insert(k_idx, [k])\n",
    "\n",
    "    knn = OneVsOneClassifier(KNeighborsClassifier(k))\n",
    "\n",
    "    knn.fit(vgg_out[\"train\"], dataset[\"train\"][\"labels\"])\n",
    "    preds = knn.predict(vgg_out[\"valid\"])\n",
    "\n",
    "    accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "    precision = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"macro\"), 3)\n",
    "\n",
    "    KNN_VGG_stats[k_idx].append((accuracy, precision))\n",
    "\n",
    "    # ConfusionMatrixDisplay(confusion_matrix(dataset['valid']['labels'], preds),display_labels=dataset['valid']['unique_labels']).plot()\n",
    "\n",
    "KNN_VGG_df = pd.DataFrame(KNN_VGG_stats, columns=[\"k\\\\VGG\", \"\"])\n",
    "display(KNN_VGG_df)\n",
    "\n",
    "for k_idx, k in enumerate(k_to_test[\"PCA\"]):\n",
    "\n",
    "    KNN_PCA_stats.insert(k_idx, [k])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"PCA\"]):\n",
    "        knn = OneVsOneClassifier(KNeighborsClassifier(k))\n",
    "\n",
    "        knn.fit(PCAs_results[\"train\"][n_components], dataset[\"train\"][\"labels\"])\n",
    "        preds = knn.predict(PCAs_results[\"valid\"][n_components])\n",
    "\n",
    "        accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "        precision = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"macro\"), 3)\n",
    "\n",
    "        KNN_PCA_stats[k_idx].insert(n_components_idx + 1, (accuracy, precision))\n",
    "\n",
    "        # ConfusionMatrixDisplay(confusion_matrix(dataset['valid']['labels'], preds),display_labels=dataset['valid']['unique_labels']).plot()\n",
    "\n",
    "KNN_PCA_df = pd.DataFrame(\n",
    "    KNN_PCA_stats, columns=[\"k\\\\PCA components\"] + n_components_to_test[\"PCA\"]\n",
    ")\n",
    "display(KNN_PCA_df)\n",
    "\n",
    "for k_idx, k in enumerate(k_to_test[\"LDA\"]):\n",
    "\n",
    "    KNN_LDA_stats.insert(k_idx, [k])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"LDA\"]):\n",
    "        knn = OneVsOneClassifier(KNeighborsClassifier(k))\n",
    "\n",
    "        knn.fit(LDAs_results[\"train\"][n_components], dataset[\"train\"][\"labels\"])\n",
    "        preds = knn.predict(LDAs_results[\"valid\"][n_components])\n",
    "\n",
    "        accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "        precision = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"macro\"), 3)\n",
    "\n",
    "        KNN_LDA_stats[k_idx].insert(n_components_idx + 1, (accuracy, precision))\n",
    "        # ConfusionMatrixDisplay(confusion_matrix(dataset['valid']['labels'], preds),display_labels=dataset['valid']['unique_labels']).plot()\n",
    "\n",
    "\n",
    "KNN_LDA_df = pd.DataFrame(\n",
    "    KNN_LDA_stats, columns=[\"k\\\\LDA components\"] + n_components_to_test[\"LDA\"]\n",
    ")\n",
    "display(KNN_LDA_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_out = pickle.load(open(\".pkl/vgg_out.pkl\", \"rb\"))\n",
    " \n",
    "PCAs_results = pickle.load(open(\".pkl/vgg_pca_out.pkl\", \"rb\"))\n",
    "LDAs_results = pickle.load(open(\".pkl/vgg_lda_out.pkl\", \"rb\"))\n",
    "TSNEs_results = pickle.load(open(\".pkl/vgg_tsne_out.pkl\", \"rb\"))\n",
    "\n",
    "kernels_to_test = {\n",
    "    \"VGG\": [\"linear\", \"poly\", \"sigmoid\"], \n",
    "    \"PCA\": [\"linear\", \"poly\", \"sigmoid\"], \n",
    "    \"LDA\": [\"linear\", \"poly\", \"sigmoid\"], \n",
    "}\n",
    "\n",
    "SVM_VGG_stats = []\n",
    "SVM_PCA_stats = []\n",
    "SVM_LDA_stats = []\n",
    "\n",
    "for kernel_idx, kernel in enumerate(kernels_to_test[\"PCA\"]):\n",
    "\n",
    "    SVM_PCA_stats.insert(kernel_idx, [kernel])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"PCA\"]):\n",
    "        svm = OneVsOneClassifier(SVC(kernel=kernel))\n",
    "\n",
    "        svm.fit(PCAs_results[\"train\"][n_components], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "        preds = svm.predict(PCAs_results[\"valid\"][n_components])\n",
    "\n",
    "        accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "        f_one_score = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"weighted\"), 3)\n",
    "\n",
    "        SVM_PCA_stats[kernel_idx].insert(n_components_idx + 1, (accuracy, f_one_score))\n",
    "\n",
    "SVM_PCA_df = pd.DataFrame(\n",
    "    SVM_PCA_stats, columns=[\"kernel\\\\PCA components\"] + n_components_to_test[\"PCA\"]\n",
    ")\n",
    "display(SVM_PCA_df)\n",
    "\n",
    "for kernel_idx, kernel in enumerate(kernels_to_test[\"LDA\"]):\n",
    "\n",
    "    SVM_LDA_stats.insert(kernel_idx, [kernel])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"LDA\"]):\n",
    "        svm = OneVsOneClassifier(SVC(kernel=kernel))\n",
    "\n",
    "        svm.fit(LDAs_results[\"train\"][n_components], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "        preds = svm.predict(LDAs_results[\"valid\"][n_components])\n",
    "\n",
    "        accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "        f_one_score = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"weighted\"), 3)\n",
    "\n",
    "        SVM_LDA_stats[kernel_idx].insert(n_components_idx + 1, (accuracy, f_one_score))\n",
    "\n",
    "SVM_LDA_df = pd.DataFrame(\n",
    "    SVM_LDA_stats, columns=[\"kernel\\\\LDA components\"] + n_components_to_test[\"LDA\"]\n",
    ")\n",
    "display(SVM_LDA_df)\n",
    "\n",
    "\n",
    "for kernel_idx, kernel in enumerate(kernels_to_test[\"VGG\"]):\n",
    "\n",
    "    SVM_VGG_stats.insert(kernel_idx, [kernel])\n",
    "\n",
    "    svm = OneVsOneClassifier(SVC(kernel=kernel))\n",
    "\n",
    "    svm.fit(vgg_out[\"train\"], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "    preds = svm.predict(vgg_out[\"valid\"])\n",
    "\n",
    "    accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "    f_one_score = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"weighted\"), 3)\n",
    "\n",
    "    SVM_VGG_stats[kernel_idx].append((accuracy, f_one_score))\n",
    "\n",
    "SVM_VGG_df = pd.DataFrame(\n",
    "    SVM_VGG_stats, columns=[\"kernel\\\\VGG components\", \"\"]\n",
    ")\n",
    "display(SVM_VGG_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_out = pickle.load(open(\".pkl/vgg_out.pkl\", \"rb\"))\n",
    " \n",
    "PCAs_results = pickle.load(open(\".pkl/vgg_pca_out.pkl\", \"rb\"))\n",
    "LDAs_results = pickle.load(open(\".pkl/vgg_lda_out.pkl\", \"rb\"))\n",
    "TSNEs_results = pickle.load(open(\".pkl/vgg_tsne_out.pkl\", \"rb\"))\n",
    "\n",
    "losses_to_test = {\n",
    "    \"VGG\": [\"modified_huber\", \"log_loss\", \"hinge\"],\n",
    "    \"PCA\": [\"modified_huber\", \"log_loss\", \"hinge\"],\n",
    "    \"LDA\": [\"modified_huber\", \"log_loss\", \"hinge\"]\n",
    "}\n",
    "\n",
    "SGD_VGG_stats = []\n",
    "SGD_PCA_stats = []\n",
    "SGD_LDA_stats = []\n",
    "\n",
    "for loss_idx, loss in enumerate(losses_to_test[\"VGG\"]):\n",
    "\n",
    "    SGD_VGG_stats.insert(loss_idx, [loss])\n",
    "\n",
    "    sgd = SGDClassifier(loss=loss, max_iter=10000)\n",
    "\n",
    "    sgd.fit(vgg_out[\"train\"], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "    preds = sgd.predict(vgg_out[\"valid\"])\n",
    "\n",
    "    accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "    f_one_score = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"weighted\"), 3)\n",
    "\n",
    "    SGD_VGG_stats[loss_idx].append((accuracy, f_one_score))\n",
    "\n",
    "    if loss == \"modified_huber\":\n",
    "        ConfusionMatrixDisplay(confusion_matrix(dataset['valid']['labels'], preds),display_labels=dataset['valid']['unique_labels']).plot() \n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "SGD_VGG_df = pd.DataFrame(\n",
    "    SGD_VGG_stats, columns=[\"loss\\\\VGG\", \"\"]\n",
    ")\n",
    "display(SGD_VGG_df)\n",
    "\n",
    "for loss_idx, loss in enumerate(losses_to_test[\"PCA\"]):\n",
    "\n",
    "    SGD_PCA_stats.insert(loss_idx, [loss])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"PCA\"]):\n",
    "        sgd = SGDClassifier(loss=loss, max_iter=10000)\n",
    "\n",
    "        sgd.fit(PCAs_results[\"train\"][n_components], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "        preds = sgd.predict(PCAs_results[\"valid\"][n_components])\n",
    "\n",
    "        accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "        f_one_score = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"macro\"), 3)\n",
    "\n",
    "        SGD_PCA_stats[loss_idx].insert(\n",
    "            n_components_idx + 1, (accuracy, f_one_score)\n",
    "        )\n",
    "\n",
    "SGD_PCA_df = pd.DataFrame(\n",
    "    SGD_PCA_stats, columns=[\"loss\\\\PCA\"] + n_components_to_test[\"PCA\"]\n",
    ")\n",
    "display(SGD_PCA_df)\n",
    "\n",
    "for loss_idx, loss in enumerate(losses_to_test[\"LDA\"]):\n",
    "\n",
    "    SGD_LDA_stats.insert(loss_idx, [loss])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"LDA\"]):\n",
    "        sgd = SGDClassifier(loss=loss, max_iter=10000)\n",
    "\n",
    "        sgd.fit(LDAs_results[\"train\"][n_components], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "        preds = sgd.predict(LDAs_results[\"valid\"][n_components])\n",
    "\n",
    "        accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "        f_one_score = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"macro\"), 3)\n",
    "\n",
    "        SGD_LDA_stats[loss_idx].insert(\n",
    "            n_components_idx + 1, (accuracy, f_one_score)\n",
    "        )\n",
    "\n",
    "SGD_LDA_df = pd.DataFrame(\n",
    "    SGD_LDA_stats, columns=[\"loss\\\\LDA\"] + n_components_to_test[\"LDA\"]\n",
    ")\n",
    "display(SGD_LDA_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_out = pickle.load(open(\".pkl/vgg_out.pkl\", \"rb\"))\n",
    " \n",
    "PCAs_results = pickle.load(open(\".pkl/vgg_pca_out.pkl\", \"rb\"))\n",
    "LDAs_results = pickle.load(open(\".pkl/vgg_lda_out.pkl\", \"rb\"))\n",
    "TSNEs_results = pickle.load(open(\".pkl/vgg_tsne_out.pkl\", \"rb\"))\n",
    "\n",
    "algos_to_test = {\n",
    "    \"VGG\": [\"lloyd\", \"elkan\"],\n",
    "    \"PCA\": [\"lloyd\", \"elkan\"],\n",
    "    \"LDA\": [\"lloyd\", \"elkan\"]\n",
    "}\n",
    "\n",
    "KMEANS_VGG_stats = []\n",
    "KMEANS_PCA_stats = []\n",
    "KMEANS_LDA_stats = []\n",
    "\n",
    "for algo_idx, algorithm in enumerate(algos_to_test[\"VGG\"]):\n",
    "\n",
    "    KMEANS_VGG_stats.insert(algo_idx, [algorithm])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=10, n_init='auto')\n",
    "\n",
    "    kmeans.fit(vgg_out[\"train\"])\n",
    "\n",
    "    preds = kmeans.predict(vgg_out[\"valid\"])\n",
    "\n",
    "    score = round(adjusted_rand_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "\n",
    "    KMEANS_VGG_stats[algo_idx].append(score)\n",
    "\n",
    "KMEANS_VGG_df = pd.DataFrame(\n",
    "    KMEANS_VGG_stats, columns=[\"algorithm\\\\VGG\", \"\"]\n",
    ")\n",
    "display(KMEANS_VGG_df)\n",
    "\n",
    "for algo_idx, algorithm in enumerate(algos_to_test[\"PCA\"]):\n",
    "\n",
    "    KMEANS_PCA_stats.insert(algo_idx, [algorithm])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"PCA\"]):\n",
    "        kmeans = KMeans(algorithm=algorithm, n_init='auto')\n",
    "\n",
    "        kmeans.fit(PCAs_results[\"train\"][n_components])\n",
    "\n",
    "        preds = kmeans.predict(PCAs_results[\"valid\"][n_components])\n",
    "\n",
    "        score = round(adjusted_rand_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "\n",
    "        KMEANS_PCA_stats[algo_idx].insert(n_components_idx + 1, score)\n",
    "\n",
    "KMEANS_PCA_df = pd.DataFrame(\n",
    "    KMEANS_PCA_stats, columns=[\"algorithm\\\\PCA\"] + n_components_to_test[\"PCA\"]\n",
    ")\n",
    "display(KMEANS_PCA_df)\n",
    "\n",
    "for algo_idx, algorithm in enumerate(algos_to_test[\"LDA\"]):\n",
    "\n",
    "    KMEANS_LDA_stats.insert(algo_idx, [algorithm])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"LDA\"]):\n",
    "        kmeans = KMeans(algorithm=algorithm, n_init='auto')\n",
    "\n",
    "        kmeans.fit(LDAs_results[\"train\"][n_components])\n",
    "\n",
    "        preds = kmeans.predict(LDAs_results[\"valid\"][n_components])\n",
    "\n",
    "        score = round(adjusted_rand_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "\n",
    "        KMEANS_LDA_stats[algo_idx].insert(n_components_idx + 1, score)\n",
    "\n",
    "KMEANS_LDA_df = pd.DataFrame(\n",
    "    KMEANS_LDA_stats, columns=[\"algorithm\\\\LDA\"] + n_components_to_test[\"LDA\"]\n",
    ")\n",
    "\n",
    "display(KMEANS_LDA_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
