{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch] using cpu\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pickle\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, adjusted_rand_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Initialing compute device (use GPU if available).\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'[torch] using {device}')\n",
    "\n",
    "plt.ion()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "raw_dataset = np.load(\"dataset_food101tiny.zip\", allow_pickle=True)\n",
    "\n",
    "dataset = {\n",
    "    \"train\": {\n",
    "        \"data\": [],\n",
    "        \"names\": [],\n",
    "        \"labels\": [],\n",
    "        \"unique_labels\": [],\n",
    "    },\n",
    "    \"valid\": {\n",
    "        \"data\": [],\n",
    "        \"names\": [],\n",
    "        \"labels\": [],\n",
    "        \"unique_labels\": [],\n",
    "    },\n",
    "}\n",
    "\n",
    "images_shape = (224, 224)\n",
    "\n",
    "# For each image we have the path from which we extract the name and the label of the image\n",
    "for dsKey in raw_dataset.keys():\n",
    "    splittedKey = dsKey.split(\"/\")\n",
    "\n",
    "    img_type = splittedKey[2]\n",
    "    img_label = splittedKey[3]\n",
    "    img_name = splittedKey[4]\n",
    "\n",
    "    img = Image.open(io.BytesIO(raw_dataset[dsKey]))\n",
    "    img = ImageOps.fit(img, images_shape, Image.Resampling.LANCZOS).convert(\"RGB\")\n",
    "\n",
    "    img_array = np.asarray(img)\n",
    "\n",
    "    dataset[img_type][\"data\"].append(img_array)\n",
    "    dataset[img_type][\"names\"].append(img_name)\n",
    "    dataset[img_type][\"labels\"].append(img_label)\n",
    "\n",
    "for img_type in dataset.keys():\n",
    "    dataset[img_type][\"data\"] = np.asarray(dataset[img_type][\"data\"])\n",
    "    dataset[img_type][\"names\"] = np.asarray(dataset[img_type][\"names\"])\n",
    "\n",
    "    dataset[img_type][\"unique_labels\"], dataset[img_type][\"labels\"] = np.unique(\n",
    "        np.asarray(dataset[img_type][\"labels\"]), return_inverse=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction using VGG\n",
    "\n",
    "Normalization mean and standard deviation are [here](https://pytorch.org/hub/pytorch_vision_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_std = [0.229, 0.224, 0.225]\n",
    "normalization_mean = [0.485, 0.456, 0.406]\n",
    "\n",
    "loader = transforms.Compose(\n",
    "    [\n",
    "        # transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=normalization_mean, std=normalization_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vgg_out = {\"train\": [], \"valid\": []}\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.vgg16(weights=models.VGG16_Weights.DEFAULT).features.to(device)\n",
    "\n",
    "for img_type in dataset.keys():\n",
    "    vgg_out[img_type] = []\n",
    "\n",
    "    for image_idx in range(dataset[img_type][\"data\"].shape[0]):\n",
    "        loaded_image = (\n",
    "            loader(dataset[img_type][\"data\"][image_idx, :]).unsqueeze(0).to(device)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            res = model(loaded_image)\n",
    "        features = res.data.detach().cpu().numpy().flatten()\n",
    "        print(f\"Extracting feature: {image_idx}/{dataset[img_type]['data'].shape[0]}\")\n",
    "\n",
    "        vgg_out[img_type].append(features)\n",
    "\n",
    "    vgg_out[img_type] = np.asarray(vgg_out[img_type])\n",
    "    print(vgg_out[img_type].shape)\n",
    "\n",
    "pickle.dump(vgg_out, open(\".pkl/vgg_out.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction using PCA, LDA and t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each method is tested with a set of numer of components\n",
    "n_components_to_test = {\n",
    "    \"PCA\": [3, 10, 50, 100, 200, 500, 1200],\n",
    "    \"LDA\": [3, 5, 7, 9],\n",
    "    \"TSNE\": [2, 3],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preload env\n",
    "vgg_out = pickle.load(open(\".pkl/vgg_out.pkl\", \"rb\"))\n",
    "\n",
    "# Results to compare the methods the number of component changes\n",
    "results_PCA = []\n",
    "results_LDA = []\n",
    "\n",
    "PCAs_instances = {}\n",
    "LDAs_instances = {}\n",
    "TSNEs_instances = {}\n",
    "\n",
    "PCAs_results = {\n",
    "    \"train\": {},\n",
    "    \"valid\": {},\n",
    "}\n",
    "\n",
    "LDAs_results = {\n",
    "    \"train\": {},\n",
    "    \"valid\": {},\n",
    "}\n",
    "\n",
    "TSNEs_results = {\n",
    "    \"train\": {},\n",
    "    \"valid\": {},\n",
    "}\n",
    "\n",
    "for n_components in n_components_to_test[\"PCA\"]:\n",
    "    print(f'[PCA] Extracting features (# components:{n_components})')\n",
    "    \n",
    "    PCAs_instances[n_components] = []\n",
    "\n",
    "    PCAs_results[\"train\"][n_components] = []\n",
    "    PCAs_results[\"valid\"][n_components] = []\n",
    "\n",
    "    PCA_instance = PCA(n_components=n_components)\n",
    "\n",
    "    PCA_instance.fit(vgg_out[\"train\"])\n",
    "\n",
    "    PCAs_results[\"train\"][n_components] = PCA_instance.transform(vgg_out[\"train\"])\n",
    "    PCAs_results[\"valid\"][n_components] = PCA_instance.transform(vgg_out[\"valid\"])\n",
    "\n",
    "    PCAs_instances[n_components] = PCA_instance\n",
    "    \n",
    "    results_PCA.append(\n",
    "        {\n",
    "            \"METHOD\": \"PCA\",\n",
    "            \"# Components\": n_components,\n",
    "            \"CHANNEL\": \"RGB\",\n",
    "            \"Explained Variance Ratio\": np.sum(\n",
    "                PCA_instance.explained_variance_ratio_, axis=0\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "for n_components in n_components_to_test[\"LDA\"]:\n",
    "    print(f'[LDA] Extracting features (# components:{n_components})')\n",
    "    \n",
    "    LDAs_instances[n_components] = []\n",
    "\n",
    "    LDAs_results[\"train\"][n_components] = []\n",
    "    LDAs_results[\"valid\"][n_components] = []\n",
    "\n",
    "    LDA_instance = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "\n",
    "    LDA_instance.fit(vgg_out[\"train\"], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "    LDAs_results[\"train\"][n_components] = LDA_instance.transform(vgg_out[\"train\"])\n",
    "    LDAs_results[\"valid\"][n_components] = LDA_instance.transform(vgg_out[\"valid\"])\n",
    "\n",
    "    LDAs_instances[n_components] = LDA_instance\n",
    "\n",
    "    results_LDA.append(\n",
    "        {\n",
    "            \"METHOD\": \"LDA\",\n",
    "            \"# Components\": n_components,\n",
    "            \"CHANNEL\": \"RGB\",\n",
    "            \"Explained Variance Ratio\": np.sum(\n",
    "                LDA_instance.explained_variance_ratio_, axis=0\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "for n_components in n_components_to_test[\"TSNE\"]:\n",
    "    print(f'[t-SNE] Extracting features (# components:{n_components})')\n",
    "\n",
    "    TSNEs_instances[n_components] = []\n",
    "\n",
    "    TSNEs_results[\"train\"][n_components] = []\n",
    "\n",
    "    TSNE_instance_train = TSNE(n_components=n_components, verbose=1, n_iter=3000)\n",
    "    TSNE_instance_valid = TSNE(n_components=n_components, verbose=1, n_iter=3000)\n",
    "\n",
    "    TSNEs_results[\"train\"][n_components] = TSNE_instance_train.fit_transform(\n",
    "        LDAs_results[\"train\"][7]\n",
    "    )\n",
    "    TSNEs_results[\"valid\"][n_components] = TSNE_instance_valid.fit_transform(\n",
    "        LDAs_results[\"valid\"][7]\n",
    "    )\n",
    "\n",
    "    TSNEs_instances[n_components] = [TSNE_instance_train, TSNE_instance_valid]\n",
    "\n",
    "\n",
    "# Pandas tables\n",
    "df_results_PCA = pd.DataFrame(results_PCA)\n",
    "df_results_LDA = pd.DataFrame(results_LDA)\n",
    "\n",
    "def highlight_cells(val):\n",
    "    color = \"\"\n",
    "    if val > 0.80:\n",
    "        color = \"background-color: lightgreen; color: black; font-weight: bold\"\n",
    "    elif val < 0.80:\n",
    "        color = \"background-color: lightcoral; color: black; font-weight: bold\"\n",
    "    return color\n",
    "\n",
    "# Apply the style\n",
    "df_results_PCA_styled = (\n",
    "    df_results_PCA.style.map(highlight_cells, subset=[\"Explained Variance Ratio\"])\n",
    "    .set_caption(\"PCA Results\")\n",
    "    .set_properties(**{\"text-align\": \"center\"})\n",
    ")\n",
    "\n",
    "df_results_LDA_styled = (\n",
    "    df_results_LDA.style.map(highlight_cells, subset=[\"Explained Variance Ratio\"])\n",
    "    .set_caption(\"LDA Results\")\n",
    "    .set_properties(**{\"text-align\": \"center\"})\n",
    ")\n",
    "\n",
    "\n",
    "pickle.dump(PCAs_results, open(\".pkl/vgg_pca_out.pkl\", \"wb\"))\n",
    "pickle.dump(LDAs_results, open(\".pkl/vgg_lda_out.pkl\", \"wb\"))\n",
    "pickle.dump(TSNEs_results, open(\".pkl/vgg_tsne_out.pkl\", \"wb\"))\n",
    "\n",
    "display(df_results_PCA_styled)\n",
    "display(df_results_LDA_styled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D/3D Data visualization using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7102d54369e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_out = pickle.load(open(\".pkl/vgg_out.pkl\", \"rb\"))\n",
    " \n",
    "PCAs_results = pickle.load(open(\".pkl/vgg_pca_out.pkl\", \"rb\"))\n",
    "LDAs_results = pickle.load(open(\".pkl/vgg_lda_out.pkl\", \"rb\"))\n",
    "TSNEs_results = pickle.load(open(\".pkl/vgg_tsne_out.pkl\", \"rb\"))\n",
    "\n",
    "tSNE_fig_2D = plt.figure()\n",
    "tSNE_3D = tSNE_fig_2D.add_subplot()\n",
    "\n",
    "for i in range(len(dataset[\"train\"][\"unique_labels\"])):\n",
    "    classIdxs = dataset[\"train\"][\"labels\"] == i\n",
    "\n",
    "    tsne_features = TSNEs_results[\"train\"][2][classIdxs, :]\n",
    "\n",
    "    tSNE_3D.set_label(dataset[\"train\"][\"unique_labels\"][i])\n",
    "    tSNE_3D.scatter(\n",
    "        tsne_features[:, 0],\n",
    "        tsne_features[:, 1],\n",
    "        marker=\".\",\n",
    "        label=dataset[\"train\"][\"unique_labels\"][i],\n",
    "    )\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# 3D plot\n",
    "tSNE_fig_3D = plt.figure()\n",
    "tSNE_3D = tSNE_fig_3D.add_subplot(projection=\"3d\")\n",
    "\n",
    "for i in range(len(dataset[\"train\"][\"unique_labels\"])):\n",
    "    classIdxs = dataset[\"train\"][\"labels\"] == i\n",
    "\n",
    "    tsne_features = TSNEs_results[\"train\"][3][classIdxs, :]\n",
    "\n",
    "    tSNE_3D.scatter(\n",
    "        tsne_features[:, 0],\n",
    "        tsne_features[:, 1],\n",
    "        tsne_features[:, 2],\n",
    "        marker=\".\",\n",
    "        label=dataset[\"train\"][\"unique_labels\"][i],\n",
    "    )\n",
    "\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_out = pickle.load(open(\".pkl/vgg_out.pkl\", \"rb\"))\n",
    " \n",
    "PCAs_results = pickle.load(open(\".pkl/vgg_pca_out.pkl\", \"rb\"))\n",
    "LDAs_results = pickle.load(open(\".pkl/vgg_lda_out.pkl\", \"rb\"))\n",
    "TSNEs_results = pickle.load(open(\".pkl/vgg_tsne_out.pkl\", \"rb\"))\n",
    "\n",
    "# Number of neighbors to test\n",
    "k_to_test = {\n",
    "    \"VGG\": [3, 5, 9, 15, 21, 55, 111, 251],\n",
    "    \"PCA\": [3, 5, 9, 15, 21, 55, 111, 251],\n",
    "    \"LDA\": [3, 5, 9, 15, 21, 55, 111, 251],\n",
    "}\n",
    "\n",
    "KNN_VGG_stats = []\n",
    "KNN_PCA_stats = []\n",
    "KNN_LDA_stats = []\n",
    "\n",
    "for k_idx, k in enumerate(k_to_test[\"VGG\"]):\n",
    "\n",
    "    KNN_VGG_stats.insert(k_idx, [k])\n",
    "\n",
    "    knn = OneVsOneClassifier(KNeighborsClassifier(k))\n",
    "\n",
    "    knn.fit(vgg_out[\"train\"], dataset[\"train\"][\"labels\"])\n",
    "    preds = knn.predict(vgg_out[\"valid\"])\n",
    "\n",
    "    accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "    precision = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"macro\"), 3)\n",
    "\n",
    "    KNN_VGG_stats[k_idx].append((accuracy, precision))\n",
    "\n",
    "    # ConfusionMatrixDisplay(confusion_matrix(dataset['valid']['labels'], preds),display_labels=dataset['valid']['unique_labels']).plot()\n",
    "\n",
    "KNN_VGG_df = pd.DataFrame(KNN_VGG_stats, columns=[\"k\\\\VGG\", \"\"])\n",
    "display(KNN_VGG_df)\n",
    "\n",
    "for k_idx, k in enumerate(k_to_test[\"PCA\"]):\n",
    "\n",
    "    KNN_PCA_stats.insert(k_idx, [k])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"PCA\"]):\n",
    "        knn = OneVsOneClassifier(KNeighborsClassifier(k))\n",
    "\n",
    "        knn.fit(PCAs_results[\"train\"][n_components], dataset[\"train\"][\"labels\"])\n",
    "        preds = knn.predict(PCAs_results[\"valid\"][n_components])\n",
    "\n",
    "        accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "        precision = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"macro\"), 3)\n",
    "\n",
    "        KNN_PCA_stats[k_idx].insert(n_components_idx + 1, (accuracy, precision))\n",
    "\n",
    "        # ConfusionMatrixDisplay(confusion_matrix(dataset['valid']['labels'], preds),display_labels=dataset['valid']['unique_labels']).plot()\n",
    "\n",
    "KNN_PCA_df = pd.DataFrame(\n",
    "    KNN_PCA_stats, columns=[\"k\\\\PCA components\"] + n_components_to_test[\"PCA\"]\n",
    ")\n",
    "display(KNN_PCA_df)\n",
    "\n",
    "for k_idx, k in enumerate(k_to_test[\"LDA\"]):\n",
    "\n",
    "    KNN_LDA_stats.insert(k_idx, [k])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"LDA\"]):\n",
    "        knn = OneVsOneClassifier(KNeighborsClassifier(k))\n",
    "\n",
    "        knn.fit(LDAs_results[\"train\"][n_components], dataset[\"train\"][\"labels\"])\n",
    "        preds = knn.predict(LDAs_results[\"valid\"][n_components])\n",
    "\n",
    "        accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "        precision = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"macro\"), 3)\n",
    "\n",
    "        KNN_LDA_stats[k_idx].insert(n_components_idx + 1, (accuracy, precision))\n",
    "        # ConfusionMatrixDisplay(confusion_matrix(dataset['valid']['labels'], preds),display_labels=dataset['valid']['unique_labels']).plot()\n",
    "\n",
    "\n",
    "KNN_LDA_df = pd.DataFrame(\n",
    "    KNN_LDA_stats, columns=[\"k\\\\LDA components\"] + n_components_to_test[\"LDA\"]\n",
    ")\n",
    "display(KNN_LDA_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_out = pickle.load(open(\".pkl/vgg_out.pkl\", \"rb\"))\n",
    " \n",
    "PCAs_results = pickle.load(open(\".pkl/vgg_pca_out.pkl\", \"rb\"))\n",
    "LDAs_results = pickle.load(open(\".pkl/vgg_lda_out.pkl\", \"rb\"))\n",
    "TSNEs_results = pickle.load(open(\".pkl/vgg_tsne_out.pkl\", \"rb\"))\n",
    "\n",
    "kernels_to_test = {\n",
    "    \"VGG\": [\"linear\", \"poly\", \"sigmoid\"], \n",
    "    \"PCA\": [\"linear\", \"poly\", \"sigmoid\"], \n",
    "    \"LDA\": [\"linear\", \"poly\", \"sigmoid\"], \n",
    "}\n",
    "\n",
    "SVM_VGG_stats = []\n",
    "SVM_PCA_stats = []\n",
    "SVM_LDA_stats = []\n",
    "\n",
    "for kernel_idx, kernel in enumerate(kernels_to_test[\"PCA\"]):\n",
    "\n",
    "    SVM_PCA_stats.insert(kernel_idx, [kernel])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"PCA\"]):\n",
    "        svm = OneVsOneClassifier(SVC(kernel=kernel))\n",
    "\n",
    "        svm.fit(PCAs_results[\"train\"][n_components], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "        preds = svm.predict(PCAs_results[\"valid\"][n_components])\n",
    "\n",
    "        accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "        f_one_score = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"weighted\"), 3)\n",
    "\n",
    "        SVM_PCA_stats[kernel_idx].insert(n_components_idx + 1, (accuracy, f_one_score))\n",
    "\n",
    "SVM_PCA_df = pd.DataFrame(\n",
    "    SVM_PCA_stats, columns=[\"kernel\\\\PCA components\"] + n_components_to_test[\"PCA\"]\n",
    ")\n",
    "display(SVM_PCA_df)\n",
    "\n",
    "for kernel_idx, kernel in enumerate(kernels_to_test[\"LDA\"]):\n",
    "\n",
    "    SVM_LDA_stats.insert(kernel_idx, [kernel])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"LDA\"]):\n",
    "        svm = OneVsOneClassifier(SVC(kernel=kernel))\n",
    "\n",
    "        svm.fit(LDAs_results[\"train\"][n_components], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "        preds = svm.predict(LDAs_results[\"valid\"][n_components])\n",
    "\n",
    "        accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "        f_one_score = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"weighted\"), 3)\n",
    "\n",
    "        SVM_LDA_stats[kernel_idx].insert(n_components_idx + 1, (accuracy, f_one_score))\n",
    "\n",
    "SVM_LDA_df = pd.DataFrame(\n",
    "    SVM_LDA_stats, columns=[\"kernel\\\\LDA components\"] + n_components_to_test[\"LDA\"]\n",
    ")\n",
    "display(SVM_LDA_df)\n",
    "\n",
    "\n",
    "for kernel_idx, kernel in enumerate(kernels_to_test[\"VGG\"]):\n",
    "\n",
    "    SVM_VGG_stats.insert(kernel_idx, [kernel])\n",
    "\n",
    "    svm = OneVsOneClassifier(SVC(kernel=kernel))\n",
    "\n",
    "    svm.fit(vgg_out[\"train\"], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "    preds = svm.predict(vgg_out[\"valid\"])\n",
    "\n",
    "    accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "    f_one_score = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"weighted\"), 3)\n",
    "\n",
    "    SVM_VGG_stats[kernel_idx].append((accuracy, f_one_score))\n",
    "\n",
    "SVM_VGG_df = pd.DataFrame(\n",
    "    SVM_VGG_stats, columns=[\"kernel\\\\VGG components\", \"\"]\n",
    ")\n",
    "display(SVM_VGG_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss\\VGG</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modified_huber</td>\n",
       "      <td>(0.804, 0.803)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>(0.804, 0.802)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hinge</td>\n",
       "      <td>(0.82, 0.819)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss\\VGG                \n",
       "0  modified_huber  (0.804, 0.803)\n",
       "1        log_loss  (0.804, 0.802)\n",
       "2           hinge   (0.82, 0.819)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss\\PCA</th>\n",
       "      <th>3</th>\n",
       "      <th>10</th>\n",
       "      <th>50</th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>500</th>\n",
       "      <th>1200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modified_huber</td>\n",
       "      <td>(0.328, 0.286)</td>\n",
       "      <td>(0.562, 0.542)</td>\n",
       "      <td>(0.74, 0.737)</td>\n",
       "      <td>(0.748, 0.744)</td>\n",
       "      <td>(0.754, 0.749)</td>\n",
       "      <td>(0.78, 0.776)</td>\n",
       "      <td>(0.798, 0.793)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>(0.326, 0.289)</td>\n",
       "      <td>(0.566, 0.558)</td>\n",
       "      <td>(0.728, 0.72)</td>\n",
       "      <td>(0.752, 0.747)</td>\n",
       "      <td>(0.76, 0.757)</td>\n",
       "      <td>(0.78, 0.775)</td>\n",
       "      <td>(0.802, 0.794)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hinge</td>\n",
       "      <td>(0.312, 0.27)</td>\n",
       "      <td>(0.544, 0.527)</td>\n",
       "      <td>(0.738, 0.734)</td>\n",
       "      <td>(0.744, 0.738)</td>\n",
       "      <td>(0.74, 0.735)</td>\n",
       "      <td>(0.77, 0.765)</td>\n",
       "      <td>(0.8, 0.794)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss\\PCA               3              10              50  \\\n",
       "0  modified_huber  (0.328, 0.286)  (0.562, 0.542)   (0.74, 0.737)   \n",
       "1        log_loss  (0.326, 0.289)  (0.566, 0.558)   (0.728, 0.72)   \n",
       "2           hinge   (0.312, 0.27)  (0.544, 0.527)  (0.738, 0.734)   \n",
       "\n",
       "              100             200            500            1200  \n",
       "0  (0.748, 0.744)  (0.754, 0.749)  (0.78, 0.776)  (0.798, 0.793)  \n",
       "1  (0.752, 0.747)   (0.76, 0.757)  (0.78, 0.775)  (0.802, 0.794)  \n",
       "2  (0.744, 0.738)   (0.74, 0.735)  (0.77, 0.765)    (0.8, 0.794)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss\\LDA</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modified_huber</td>\n",
       "      <td>(0.416, 0.397)</td>\n",
       "      <td>(0.572, 0.57)</td>\n",
       "      <td>(0.544, 0.515)</td>\n",
       "      <td>(0.628, 0.603)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>(0.494, 0.438)</td>\n",
       "      <td>(0.62, 0.608)</td>\n",
       "      <td>(0.734, 0.728)</td>\n",
       "      <td>(0.748, 0.745)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hinge</td>\n",
       "      <td>(0.426, 0.437)</td>\n",
       "      <td>(0.598, 0.593)</td>\n",
       "      <td>(0.684, 0.655)</td>\n",
       "      <td>(0.77, 0.765)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss\\LDA               3               5               7  \\\n",
       "0  modified_huber  (0.416, 0.397)   (0.572, 0.57)  (0.544, 0.515)   \n",
       "1        log_loss  (0.494, 0.438)   (0.62, 0.608)  (0.734, 0.728)   \n",
       "2           hinge  (0.426, 0.437)  (0.598, 0.593)  (0.684, 0.655)   \n",
       "\n",
       "                9  \n",
       "0  (0.628, 0.603)  \n",
       "1  (0.748, 0.745)  \n",
       "2   (0.77, 0.765)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vgg_out = pickle.load(open(\".pkl/vgg_out.pkl\", \"rb\"))\n",
    " \n",
    "PCAs_results = pickle.load(open(\".pkl/vgg_pca_out.pkl\", \"rb\"))\n",
    "LDAs_results = pickle.load(open(\".pkl/vgg_lda_out.pkl\", \"rb\"))\n",
    "TSNEs_results = pickle.load(open(\".pkl/vgg_tsne_out.pkl\", \"rb\"))\n",
    "\n",
    "losses_to_test = {\n",
    "    \"VGG\": [\"modified_huber\", \"log_loss\", \"hinge\"],\n",
    "    \"PCA\": [\"modified_huber\", \"log_loss\", \"hinge\"],\n",
    "    \"LDA\": [\"modified_huber\", \"log_loss\", \"hinge\"]\n",
    "}\n",
    "\n",
    "SGD_VGG_stats = []\n",
    "SGD_PCA_stats = []\n",
    "SGD_LDA_stats = []\n",
    "\n",
    "for loss_idx, loss in enumerate(losses_to_test[\"VGG\"]):\n",
    "\n",
    "    SGD_VGG_stats.insert(loss_idx, [loss])\n",
    "\n",
    "    sgd = SGDClassifier(loss=loss, max_iter=10000)\n",
    "\n",
    "    sgd.fit(vgg_out[\"train\"], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "    preds = sgd.predict(vgg_out[\"valid\"])\n",
    "\n",
    "    accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "    f_one_score = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"weighted\"), 3)\n",
    "\n",
    "    SGD_VGG_stats[loss_idx].append((accuracy, f_one_score))\n",
    "\n",
    "    if loss == \"modified_huber\":\n",
    "        ConfusionMatrixDisplay(confusion_matrix(dataset['valid']['labels'], preds),display_labels=dataset['valid']['unique_labels']).plot() \n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "SGD_VGG_df = pd.DataFrame(\n",
    "    SGD_VGG_stats, columns=[\"loss\\\\VGG\", \"\"]\n",
    ")\n",
    "display(SGD_VGG_df)\n",
    "\n",
    "for loss_idx, loss in enumerate(losses_to_test[\"PCA\"]):\n",
    "\n",
    "    SGD_PCA_stats.insert(loss_idx, [loss])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"PCA\"]):\n",
    "        sgd = SGDClassifier(loss=loss, max_iter=10000)\n",
    "\n",
    "        sgd.fit(PCAs_results[\"train\"][n_components], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "        preds = sgd.predict(PCAs_results[\"valid\"][n_components])\n",
    "\n",
    "        accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "        f_one_score = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"macro\"), 3)\n",
    "\n",
    "        SGD_PCA_stats[loss_idx].insert(\n",
    "            n_components_idx + 1, (accuracy, f_one_score)\n",
    "        )\n",
    "\n",
    "SGD_PCA_df = pd.DataFrame(\n",
    "    SGD_PCA_stats, columns=[\"loss\\\\PCA\"] + n_components_to_test[\"PCA\"]\n",
    ")\n",
    "display(SGD_PCA_df)\n",
    "\n",
    "for loss_idx, loss in enumerate(losses_to_test[\"LDA\"]):\n",
    "\n",
    "    SGD_LDA_stats.insert(loss_idx, [loss])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"LDA\"]):\n",
    "        sgd = SGDClassifier(loss=loss, max_iter=10000)\n",
    "\n",
    "        sgd.fit(LDAs_results[\"train\"][n_components], dataset[\"train\"][\"labels\"])\n",
    "\n",
    "        preds = sgd.predict(LDAs_results[\"valid\"][n_components])\n",
    "\n",
    "        accuracy = round(accuracy_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "        f_one_score = round(f1_score(dataset[\"valid\"][\"labels\"], preds, average=\"macro\"), 3)\n",
    "\n",
    "        SGD_LDA_stats[loss_idx].insert(\n",
    "            n_components_idx + 1, (accuracy, f_one_score)\n",
    "        )\n",
    "\n",
    "SGD_LDA_df = pd.DataFrame(\n",
    "    SGD_LDA_stats, columns=[\"loss\\\\LDA\"] + n_components_to_test[\"LDA\"]\n",
    ")\n",
    "display(SGD_LDA_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm\\VGG</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lloyd</td>\n",
       "      <td>0.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elkan</td>\n",
       "      <td>0.103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  algorithm\\VGG       \n",
       "0         lloyd  0.194\n",
       "1         elkan  0.103"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm\\PCA</th>\n",
       "      <th>3</th>\n",
       "      <th>10</th>\n",
       "      <th>50</th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>500</th>\n",
       "      <th>1200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lloyd</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elkan</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  algorithm\\PCA      3     10     50    100    200    500   1200\n",
       "0         lloyd  0.146  0.170  0.133  0.140  0.190  0.132  0.120\n",
       "1         elkan  0.144  0.185  0.175  0.172  0.166  0.143  0.119"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm\\LDA</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lloyd</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elkan</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  algorithm\\LDA      3      5      7      9\n",
       "0         lloyd  0.253  0.331  0.467  0.427\n",
       "1         elkan  0.223  0.374  0.446  0.494"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vgg_out = pickle.load(open(\".pkl/vgg_out.pkl\", \"rb\"))\n",
    " \n",
    "PCAs_results = pickle.load(open(\".pkl/vgg_pca_out.pkl\", \"rb\"))\n",
    "LDAs_results = pickle.load(open(\".pkl/vgg_lda_out.pkl\", \"rb\"))\n",
    "TSNEs_results = pickle.load(open(\".pkl/vgg_tsne_out.pkl\", \"rb\"))\n",
    "\n",
    "algos_to_test = {\n",
    "    \"VGG\": [\"lloyd\", \"elkan\"],\n",
    "    \"PCA\": [\"lloyd\", \"elkan\"],\n",
    "    \"LDA\": [\"lloyd\", \"elkan\"]\n",
    "}\n",
    "\n",
    "KMEANS_VGG_stats = []\n",
    "KMEANS_PCA_stats = []\n",
    "KMEANS_LDA_stats = []\n",
    "\n",
    "for algo_idx, algorithm in enumerate(algos_to_test[\"VGG\"]):\n",
    "\n",
    "    KMEANS_VGG_stats.insert(algo_idx, [algorithm])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=10, n_init='auto')\n",
    "\n",
    "    kmeans.fit(vgg_out[\"train\"])\n",
    "\n",
    "    preds = kmeans.predict(vgg_out[\"valid\"])\n",
    "\n",
    "    score = round(adjusted_rand_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "\n",
    "    KMEANS_VGG_stats[algo_idx].append(score)\n",
    "\n",
    "KMEANS_VGG_df = pd.DataFrame(\n",
    "    KMEANS_VGG_stats, columns=[\"algorithm\\\\VGG\", \"\"]\n",
    ")\n",
    "display(KMEANS_VGG_df)\n",
    "\n",
    "for algo_idx, algorithm in enumerate(algos_to_test[\"PCA\"]):\n",
    "\n",
    "    KMEANS_PCA_stats.insert(algo_idx, [algorithm])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"PCA\"]):\n",
    "        kmeans = KMeans(algorithm=algorithm, n_init='auto')\n",
    "\n",
    "        kmeans.fit(PCAs_results[\"train\"][n_components])\n",
    "\n",
    "        preds = kmeans.predict(PCAs_results[\"valid\"][n_components])\n",
    "\n",
    "        score = round(adjusted_rand_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "\n",
    "        KMEANS_PCA_stats[algo_idx].insert(n_components_idx + 1, score)\n",
    "\n",
    "KMEANS_PCA_df = pd.DataFrame(\n",
    "    KMEANS_PCA_stats, columns=[\"algorithm\\\\PCA\"] + n_components_to_test[\"PCA\"]\n",
    ")\n",
    "display(KMEANS_PCA_df)\n",
    "\n",
    "for algo_idx, algorithm in enumerate(algos_to_test[\"LDA\"]):\n",
    "\n",
    "    KMEANS_LDA_stats.insert(algo_idx, [algorithm])\n",
    "\n",
    "    for n_components_idx, n_components in enumerate(n_components_to_test[\"LDA\"]):\n",
    "        kmeans = KMeans(algorithm=algorithm, n_init='auto')\n",
    "\n",
    "        kmeans.fit(LDAs_results[\"train\"][n_components])\n",
    "\n",
    "        preds = kmeans.predict(LDAs_results[\"valid\"][n_components])\n",
    "\n",
    "        score = round(adjusted_rand_score(dataset[\"valid\"][\"labels\"], preds), 3)\n",
    "\n",
    "        KMEANS_LDA_stats[algo_idx].insert(n_components_idx + 1, score)\n",
    "\n",
    "KMEANS_LDA_df = pd.DataFrame(\n",
    "    KMEANS_LDA_stats, columns=[\"algorithm\\\\LDA\"] + n_components_to_test[\"LDA\"]\n",
    ")\n",
    "\n",
    "display(KMEANS_LDA_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
